# Michael

It seems the performance is quite similar. I am thinking that perhaps reward supervision of the world model is good enough. Maintaining the point that learning an unsupervised world model is more useful than a task-specific one, maybe I can take the "task-specific" part of the supervision out of the mix. That is to say, we will still have a Q-function, but it will not be used to supervise the world model.

What this wants to show is that we can have the world model, and we can have the policy, but they can be separate entities, implying that the world model itself is highly scalable, while the policy can be added with just a few examples.

python -m scripts.tdmpc2.train task=hopper-hop model_size=5 batch_size=256 action_inference_coef=1 obs=rgb num_envs=1 eval_episodes=10 'wandb_name="v03,action_inference_coef=1,obs=rgb"' stopgrad_reward_and_q=true
python -m scripts.tdmpc2.train task=hopper-hop model_size=5 batch_size=256 action_inference_coef=0 obs=rgb num_envs=1 eval_episodes=10 'wandb_name="v03,action_inference_coef=0,obs=rgb"' stopgrad_reward_and_q=true
