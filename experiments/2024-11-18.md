# Michael

It seems that when you take out task-specific supervision of the world model (beyond the policy I guess?), the model performs worse (as expected).
However, by adding this action-centric supervision, we can create decent models anyway.

Trying on a Metaworld environment (am not running this yet):
python -m scripts.tdmpc2.train task=mw-hammer model_size=5 batch_size=256 action_inference_coef=1 obs=rgb num_envs=1 eval_episodes=10 'wandb_name="v03,action_inference_coef=1,obs=rgb,task=mw-hammer"' stopgrad_reward_and_q=true 'episode_save_dir="/scratch/gsk6me/robotics-research/uva-learning-for-interactive-robotics-project/episodes/v03,action_inference_coef=1,obs=rgb,task=mw-hammer"'

RUNNING (1):
python -m scripts.tdmpc2.train task=hopper-hop model_size=5 batch_size=256 action_inference_coef=1 obs=rgb num_envs=1 eval_episodes=10 'wandb_name="v03,action_inference_coef=1,obs=rgb,task=hopper-hop"' stopgrad_reward_and_q=true 'episode_save_dir="/scratch/gsk6me/robotics-research/uva-learning-for-interactive-robotics-project/episodes/v03,action_inference_coef=1,obs=rgb,task=hopper-hop"'

RUNNING (2):
python -m scripts.tdmpc2.train task=quadruped_run model_size=5 batch_size=256 action_inference_coef=1 obs=rgb num_envs=1 eval_episodes=10 'wandb_name="v03,action_inference_coef=1,obs=rgb,task=quadruped_run"' stopgrad_reward_and_q=true 'episode_save_dir="/scratch/gsk6me/robotics-research/uva-learning-for-interactive-robotics-project/episodes/v03,action_inference_coef=1,obs=rgb,task=quadruped_run"'
