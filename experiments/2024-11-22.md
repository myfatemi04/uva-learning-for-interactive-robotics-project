# Michael

Going to reduce the "action inference coef" to 0.1 and 0.5, to see if there are any changes to convergence. Otherwise, will stop from here any run main experiments.

python -m scripts.tdmpc2.train task=hopper-hop model_size=5 batch_size=256 action_inference_coef=0.1 obs=rgb num_envs=1 eval_episodes=10 'wandb_name="v04,action_inference_coef=0.1,obs=rgb,task=hopper-hop"' stopgrad_reward_and_q=true 'work_dir="/scratch/gsk6me/robotics-research/uva-learning-for-interactive-robotics-project/episodes/v04,action_inference_coef=0.1,obs=rgb,task=hopper-hop"'

python -m scripts.tdmpc2.train task=hopper-hop model_size=5 batch_size=256 action_inference_coef=0.5 obs=rgb num_envs=1 eval_episodes=10 'wandb_name="v04,action_inference_coef=0.5,obs=rgb,task=hopper-hop"' stopgrad_reward_and_q=true 'work_dir="/scratch/gsk6me/robotics-research/uva-learning-for-interactive-robotics-project/episodes/v04,action_inference_coef=0.5,obs=rgb,task=hopper-hop"'

Trying these experiments on the CS server, for hopefully faster training.

RUNNING (1) (Fixing action inference coef):
python -m scripts.tdmpc2.train task=hopper-hop model_size=5 batch_size=256 action_inference_coef=1 obs=rgb num_envs=1 eval_episodes=10 'wandb_name="v04-cs-server,action_inference_coef=1,obs=rgb,task=hopper-hop"' stopgrad_reward_and_q=true 'work_dir="/u/gsk6me/robo/repo/episodes/v04-cs-server,action_inference_coef=1,obs=rgb,task=hopper-hop"'

RUNNING (2) (walker-walk environment):
python -m scripts.tdmpc2.train task=walker-walk model_size=5 batch_size=256 action_inference_coef=1 obs=rgb num_envs=1 eval_episodes=10 'wandb_name="v04,action_inference_coef=1,obs=rgb,task=walker-walk"' stopgrad_reward_and_q=true 'work_dir="/u/gsk6me/robo/repoepisodes/v04,action_inference_coef=1,obs=rgb,task=walker-walk"'
