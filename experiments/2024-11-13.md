# Michael

After fixing the stopgrad issue, there seems to be an improvement. However, the original TDMPC paper is able to train much more quickly than this. The use the following hyperparameters:
- No vectorization
- Batch size of 256 (for single-task experiments)
- Batch size of 1024 (for multi-task experiments)

python -m scripts.tdmpc2.train task=hopper-hop model_size=5 batch_size=256 action_inference_coef=1 obs=rgb   num_envs=1 eval_episodes=10 'wandb_name="v02.1,action_inference_coef=1,obs=rgb"'
python -m scripts.tdmpc2.train task=hopper-hop model_size=5 batch_size=256 action_inference_coef=0 obs=rgb   num_envs=1 eval_episodes=10 'wandb_name="v02.1,action_inference_coef=0,obs=rgb"'
