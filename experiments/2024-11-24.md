# Michael

Testing residual...

python -m scripts.tdmpc2.train task=hopper-hop model_size=5 batch_size=256 action_inference_coef=1 obs=rgb num_envs=1 eval_episodes=10 'wandb_name="v05,task=hopper-hop"' stopgrad_reward_and_q=true 'work_dir="/scratch/gsk6me/robotics-research/uva-learning-for-interactive-robotics-project/episodes/v05,task=hopper-hop"'

Testing cross-task...

walker-run:
WITH INV DYN PRETRAINING:
python -m scripts.tdmpc2.train \
    task=walker-run \
    'encoder_and_dynamics_checkpoint="/scratch/gsk6me/robotics-research/uva-learning-for-interactive-robotics-project/episodes/v04,action_inference_coef=1,obs=rgb,task=walker-walk/latest.pt"' \
    encoder_and_dynamics_freeze=true \
    'wandb_name="v05,task=walker-run,from=walker-walk-invdyn"'

WITHOUT INV DYN PRETRAINING:
python -m scripts.tdmpc2.train \
    task=walker-run \
    'wandb_name="v05,task=walker-run"'

Training models with full reward assumed ...

walker-walk:
python -m scripts.tdmpc2.train \
    task=walker-walk \
    action_inference_coef=0 \
    stopgrad_reward_and_q=false \
    'wandb_name="v05,task=walker-walk,rewardonly"'

hopper-hop:
python -m scripts.tdmpc2.train \
    task=hopper-hop \
    action_inference_coef=0 \
    stopgrad_reward_and_q=false \
    'wandb_name="v05,task=hopper-hop,rewardonly"'

walker-run:
python -m scripts.tdmpc2.train \
    task=walker-run \
    action_inference_coef=0 \
    stopgrad_reward_and_q=false \
    'wandb_name="v05,task=walker-run,rewardonly"'
